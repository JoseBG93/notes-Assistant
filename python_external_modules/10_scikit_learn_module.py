#!/usr/bin/env python3
"""
M√ìDULO EXTERNO: scikit-learn
============================

¬øQU√â ES?
Scikit-learn es LA librer√≠a de machine learning m√°s popular de Python.
Proporciona algoritmos listos para usar con API consistente y simple.

INSTALACI√ìN:
pip install scikit-learn

¬øPARA QU√â SIRVE?
- Clasificaci√≥n (predecir categor√≠as)
- Regresi√≥n (predecir valores num√©ricos)
- Clustering (agrupar datos similares)
- Preprocessing (preparar datos)
- Model selection (elegir mejor modelo)
- M√©tricas de evaluaci√≥n

IMPORTANCIA: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Esencial para ML)
"""

def verificar_instalacion():
    """Verificar si scikit-learn est√° instalado"""
    try:
        import sklearn
        print("‚úÖ M√≥dulo 'scikit-learn' instalado correctamente")
        print(f"üì¶ Versi√≥n: {sklearn.__version__}")
        return True
    except ImportError:
        print("‚ùå M√≥dulo 'scikit-learn' no encontrado")
        print("üí° Para instalar: pip install scikit-learn")
        return False

def ejemplo_sklearn_basico():
    """Ejemplo b√°sico de scikit-learn"""
    
    print("=" * 50)
    print("ü§ñ M√ìDULO SCIKIT-LEARN - MACHINE LEARNING")
    print("=" * 50)
    
    if not verificar_instalacion():
        return
    
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    
    print("\nüìù Ejemplo b√°sico de regresi√≥n lineal:")
    print("""
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 1. Crear datos
X = np.random.rand(100, 1) * 10
y = 2 * X.ravel() + 1 + np.random.randn(100) * 2

# 2. Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. Crear y entrenar modelo
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# 4. Hacer predicciones
y_pred = modelo.predict(X_test)

# 5. Evaluar
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
    """)
    
    # Ejecutar ejemplo
    np.random.seed(42)
    
    # 1. Crear datos sint√©ticos
    X = np.random.rand(100, 1) * 10  # Features (variables independientes)
    y = 2 * X.ravel() + 1 + np.random.randn(100) * 2  # Target (variable dependiente)
    
    # 2. Dividir datos
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 3. Crear y entrenar modelo
    modelo = LinearRegression()
    modelo.fit(X_train, y_train)
    
    # 4. Hacer predicciones
    y_pred = modelo.predict(X_test)
    
    # 5. Evaluar modelo
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print("\n‚úÖ Resultados del modelo:")
    print(f"   üìä Datos de entrenamiento: {len(X_train)} muestras")
    print(f"   üìä Datos de prueba: {len(X_test)} muestras")
    print(f"   üìà Coeficiente (pendiente): {modelo.coef_[0]:.2f}")
    print(f"   üìà Intercepto: {modelo.intercept_:.2f}")
    print(f"   üìä Error cuadr√°tico medio: {mse:.2f}")
    print(f"   üìä R¬≤ Score: {r2:.3f}")
    
    print("\nüí° Interpretaci√≥n:")
    print(f"   ‚Ä¢ El modelo explica {r2*100:.1f}% de la varianza")
    print(f"   ‚Ä¢ Ecuaci√≥n: y = {modelo.coef_[0]:.2f}x + {modelo.intercept_:.2f}")

def ejemplo_sklearn_clasificacion():
    """Ejemplo de clasificaci√≥n con scikit-learn"""
    
    print("\n" + "=" * 50)
    print("üéØ CLASIFICACI√ìN CON SCIKIT-LEARN")
    print("=" * 50)
    
    import numpy as np
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    
    # 1. CREAR DATASET SINT√âTICO
    print("\n1Ô∏è‚É£ CREAR DATASET:")
    
    X, y = make_classification(
        n_samples=1000,      # 1000 muestras
        n_features=20,       # 20 caracter√≠sticas
        n_informative=15,    # 15 caracter√≠sticas √∫tiles
        n_redundant=5,       # 5 caracter√≠sticas redundantes
        n_classes=3,         # 3 clases
        random_state=42
    )
    
    print(f"   üìä Shape de X: {X.shape}")
    print(f"   üìä Shape de y: {y.shape}")
    print(f"   üìä Clases √∫nicas: {np.unique(y)}")
    print(f"   üìä Distribuci√≥n de clases: {np.bincount(y)}")
    
    # 2. DIVIDIR DATOS
    print("\n2Ô∏è‚É£ DIVIDIR DATOS:")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   üìà Entrenamiento: {X_train.shape[0]} muestras")
    print(f"   üìà Prueba: {X_test.shape[0]} muestras")
    
    # 3. ENTRENAR M√öLTIPLES MODELOS
    print("\n3Ô∏è‚É£ ENTRENAR MODELOS:")
    
    modelos = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42)
    }
    
    resultados = {}
    
    for nombre, modelo in modelos.items():
        # Entrenar
        modelo.fit(X_train, y_train)
        
        # Predecir
        y_pred = modelo.predict(X_test)
        
        # Evaluar
        accuracy = accuracy_score(y_test, y_pred)
        resultados[nombre] = accuracy
        
        print(f"   ü§ñ {nombre}: {accuracy:.3f} accuracy")
    
    # 4. MEJOR MODELO
    mejor_modelo = max(resultados, key=resultados.get)
    print(f"\nüèÜ Mejor modelo: {mejor_modelo} ({resultados[mejor_modelo]:.3f})")
    
    # 5. AN√ÅLISIS DETALLADO DEL MEJOR MODELO
    print("\n4Ô∏è‚É£ AN√ÅLISIS DETALLADO:")
    
    modelo_final = modelos[mejor_modelo]
    y_pred_final = modelo_final.predict(X_test)
    
    print("   üìä Classification Report:")
    print(classification_report(y_test, y_pred_final, target_names=['Clase 0', 'Clase 1', 'Clase 2']))
    
    print("   üìä Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred_final)
    print(cm)

def ejemplo_sklearn_preprocessing():
    """Ejemplo de preprocessing de datos"""
    
    print("\n" + "=" * 50)
    print("üõ†Ô∏è PREPROCESSING DE DATOS")
    print("=" * 50)
    
    import numpy as np
    import pandas as pd
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.impute import SimpleImputer
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    
    # 1. CREAR DATOS CON PROBLEMAS COMUNES
    print("\n1Ô∏è‚É£ DATOS CON PROBLEMAS T√çPICOS:")
    
    np.random.seed(42)
    
    # Datos simulados con diferentes escalas y valores faltantes
    datos = {
        'edad': np.random.randint(18, 80, 100),
        'salario': np.random.normal(50000, 15000, 100),
        'experiencia': np.random.exponential(5, 100),
        'categoria': np.random.choice(['A', 'B', 'C'], 100),
        'tiene_coche': np.random.choice([True, False], 100)
    }
    
    df = pd.DataFrame(datos)
    
    # Introducir valores faltantes
    df.loc[np.random.choice(df.index, 10), 'salario'] = np.nan
    df.loc[np.random.choice(df.index, 5), 'experiencia'] = np.nan
    
    print(f"   üìä Shape original: {df.shape}")
    print(f"   üìä Valores faltantes por columna:")
    print(df.isnull().sum())
    print(f"   üìä Tipos de datos:")
    print(df.dtypes)
    
    # 2. MANEJO DE VALORES FALTANTES
    print("\n2Ô∏è‚É£ MANEJO DE VALORES FALTANTES:")
    
    # Imputar valores faltantes
    imputer_num = SimpleImputer(strategy='mean')
    df[['salario', 'experiencia']] = imputer_num.fit_transform(df[['salario', 'experiencia']])
    
    print(f"   ‚úÖ Valores faltantes despu√©s de imputaci√≥n:")
    print(df.isnull().sum())
    
    # 3. ESCALADO DE CARACTER√çSTICAS NUM√âRICAS
    print("\n3Ô∏è‚É£ ESCALADO DE CARACTER√çSTICAS:")
    
    # Antes del escalado
    print(f"   üìä Estad√≠sticas antes del escalado:")
    print(df[['edad', 'salario', 'experiencia']].describe())
    
    # StandardScaler (media=0, std=1)
    scaler = StandardScaler()
    df_scaled = df.copy()
    df_scaled[['edad', 'salario', 'experiencia']] = scaler.fit_transform(
        df[['edad', 'salario', 'experiencia']]
    )
    
    print(f"   üìä Estad√≠sticas despu√©s del escalado:")
    print(df_scaled[['edad', 'salario', 'experiencia']].describe())
    
    # 4. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS
    print("\n4Ô∏è‚É£ CODIFICACI√ìN CATEG√ìRICAS:")
    
    # Label Encoding para variables binarias
    le = LabelEncoder()
    df_scaled['tiene_coche_encoded'] = le.fit_transform(df_scaled['tiene_coche'])
    
    # One-Hot Encoding para variables categ√≥ricas
    encoder = OneHotEncoder(sparse_output=False, drop='first')
    categoria_encoded = encoder.fit_transform(df[['categoria']])
    categoria_columns = [f'categoria_{cat}' for cat in encoder.categories_[0][1:]]
    
    for i, col in enumerate(categoria_columns):
        df_scaled[col] = categoria_encoded[:, i]
    
    print(f"   ‚úÖ Columnas despu√©s de encoding:")
    print(list(df_scaled.columns))
    
    # 5. PIPELINE COMPLETO
    print("\n5Ô∏è‚É£ PIPELINE COMPLETO:")
    
    # Crear pipeline que hace todo autom√°ticamente
    numeric_features = ['edad', 'salario', 'experiencia']
    categorical_features = ['categoria']
    
    # Pipeline para features num√©ricas
    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    
    # Pipeline para features categ√≥ricas
    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('encoder', OneHotEncoder(drop='first', sparse_output=False))
    ])
    
    # Combinar pipelines
    preprocessor = ColumnTransformer([
        ('num', numeric_pipeline, numeric_features),
        ('cat', categorical_pipeline, categorical_features)
    ])
    
    # Aplicar preprocessing
    X_processed = preprocessor.fit_transform(df[numeric_features + categorical_features])
    
    print(f"   ‚úÖ Shape despu√©s del preprocessing: {X_processed.shape}")
    print(f"   ‚úÖ Pipeline creado y aplicado exitosamente")

def ejemplo_sklearn_clustering():
    """Ejemplo de clustering (aprendizaje no supervisado)"""
    
    print("\n" + "=" * 50)
    print("üîç CLUSTERING - APRENDIZAJE NO SUPERVISADO")
    print("=" * 50)
    
    import numpy as np
    from sklearn.datasets import make_blobs
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score, adjusted_rand_score
    
    # 1. CREAR DATOS PARA CLUSTERING
    print("\n1Ô∏è‚É£ CREAR DATOS:")
    
    X, y_true = make_blobs(
        n_samples=300,
        centers=4,
        cluster_std=0.6,
        random_state=42
    )
    
    print(f"   üìä Datos creados: {X.shape}")
    print(f"   üìä Clusters reales: {len(np.unique(y_true))}")
    
    # 2. K-MEANS CLUSTERING
    print("\n2Ô∏è‚É£ K-MEANS CLUSTERING:")
    
    # Probar diferentes n√∫meros de clusters
    silhouette_scores = []
    K_range = range(2, 8)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        silhouette = silhouette_score(X, labels)
        silhouette_scores.append(silhouette)
        print(f"   üî¢ K={k}: Silhouette Score = {silhouette:.3f}")
    
    # Mejor K seg√∫n silhouette score
    mejor_k = K_range[np.argmax(silhouette_scores)]
    print(f"   üèÜ Mejor K: {mejor_k} (Score: {max(silhouette_scores):.3f})")
    
    # 3. CLUSTERING FINAL
    print("\n3Ô∏è‚É£ CLUSTERING FINAL:")
    
    kmeans_final = KMeans(n_clusters=mejor_k, random_state=42, n_init=10)
    labels_kmeans = kmeans_final.fit_predict(X)
    
    # DBSCAN como alternativa
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    dbscan = DBSCAN(eps=0.3, min_samples=5)
    labels_dbscan = dbscan.fit_predict(X_scaled)
    
    print(f"   üìä K-Means clusters encontrados: {len(np.unique(labels_kmeans))}")
    print(f"   üìä DBSCAN clusters encontrados: {len(np.unique(labels_dbscan[labels_dbscan != -1]))}")
    print(f"   üìä DBSCAN outliers: {np.sum(labels_dbscan == -1)}")
    
    # 4. EVALUACI√ìN
    print("\n4Ô∏è‚É£ EVALUACI√ìN:")
    
    # Comparar con clusters reales (si los conocemos)
    ari_kmeans = adjusted_rand_score(y_true, labels_kmeans)
    ari_dbscan = adjusted_rand_score(y_true, labels_dbscan)
    
    print(f"   üìä K-Means ARI: {ari_kmeans:.3f}")
    print(f"   üìä DBSCAN ARI: {ari_dbscan:.3f}")
    
    # Silhouette scores
    sil_kmeans = silhouette_score(X, labels_kmeans)
    sil_dbscan = silhouette_score(X, labels_dbscan) if len(np.unique(labels_dbscan)) > 1 else 0
    
    print(f"   üìä K-Means Silhouette: {sil_kmeans:.3f}")
    print(f"   üìä DBSCAN Silhouette: {sil_dbscan:.3f}")

def ejemplo_sklearn_validacion():
    """Ejemplo de validaci√≥n de modelos"""
    
    print("\n" + "=" * 50)
    print("‚úÖ VALIDACI√ìN DE MODELOS")
    print("=" * 50)
    
    import numpy as np
    from sklearn.datasets import make_classification
    from sklearn.model_selection import cross_val_score, GridSearchCV, validation_curve
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.model_selection import train_test_split
    
    # 1. CREAR DATOS
    print("\n1Ô∏è‚É£ PREPARAR DATOS:")
    
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        n_classes=2,
        random_state=42
    )
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"   üìä Datos de entrenamiento: {X_train.shape}")
    print(f"   üìä Datos de prueba: {X_test.shape}")
    
    # 2. VALIDACI√ìN CRUZADA
    print("\n2Ô∏è‚É£ VALIDACI√ìN CRUZADA:")
    
    modelo = RandomForestClassifier(random_state=42)
    
    # Cross-validation con diferentes m√©tricas
    cv_scores = cross_val_score(modelo, X_train, y_train, cv=5)
    cv_precision = cross_val_score(modelo, X_train, y_train, cv=5, scoring='precision')
    cv_recall = cross_val_score(modelo, X_train, y_train, cv=5, scoring='recall')
    cv_f1 = cross_val_score(modelo, X_train, y_train, cv=5, scoring='f1')
    
    print(f"   üìä Accuracy CV: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
    print(f"   üìä Precision CV: {cv_precision.mean():.3f} ¬± {cv_precision.std():.3f}")
    print(f"   üìä Recall CV: {cv_recall.mean():.3f} ¬± {cv_recall.std():.3f}")
    print(f"   üìä F1 CV: {cv_f1.mean():.3f} ¬± {cv_f1.std():.3f}")
    
    # 3. GRID SEARCH PARA HIPERPAR√ÅMETROS
    print("\n3Ô∏è‚É£ GRID SEARCH:")
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid,
        cv=3,  # Reducido para velocidad
        scoring='accuracy',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"   üèÜ Mejores par√°metros: {grid_search.best_params_}")
    print(f"   üìä Mejor score CV: {grid_search.best_score_:.3f}")
    
    # 4. EVALUACI√ìN FINAL
    print("\n4Ô∏è‚É£ EVALUACI√ìN FINAL:")
    
    # Modelo con mejores par√°metros
    mejor_modelo = grid_search.best_estimator_
    y_pred = mejor_modelo.predict(X_test)
    
    # M√©tricas en conjunto de prueba
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    print(f"   üìä Test Accuracy: {accuracy:.3f}")
    print(f"   üìä Test Precision: {precision:.3f}")
    print(f"   üìä Test Recall: {recall:.3f}")
    print(f"   üìä Test F1: {f1:.3f}")
    
    # 5. IMPORTANCIA DE CARACTER√çSTICAS
    print("\n5Ô∏è‚É£ IMPORTANCIA DE CARACTER√çSTICAS:")
    
    feature_importance = mejor_modelo.feature_importances_
    indices_importantes = np.argsort(feature_importance)[::-1][:5]
    
    print(f"   üìä Top 5 caracter√≠sticas m√°s importantes:")
    for i, idx in enumerate(indices_importantes):
        print(f"      {i+1}. Feature {idx}: {feature_importance[idx]:.3f}")

def integracion_con_notesassistant():
    """Ejemplo de integraci√≥n con proyecto notesAssistant"""
    
    print("\n" + "=" * 50)
    print("üóÇÔ∏è INTEGRACI√ìN CON NOTESASSISTANT")
    print("=" * 50)
    
    import numpy as np
    import pandas as pd
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    
    print("üí° ML aplicado a tu proyecto de notas:")
    
    # 1. CLASIFICACI√ìN AUTOM√ÅTICA DE NOTAS
    print("\n1Ô∏è‚É£ CLASIFICACI√ìN AUTOM√ÅTICA DE NOTAS:")
    
    # Datos simulados de notas
    notas_ejemplo = [
        "Reuni√≥n con el equipo ma√±ana a las 10am",
        "Comprar leche, pan y huevos en el supermercado", 
        "Estudiar cap√≠tulo 5 de √°lgebra lineal",
        "Llamar al m√©dico para agendar cita",
        "Idea para nueva funcionalidad en la app",
        "Revisar c√≥digo del m√≥dulo de autenticaci√≥n",
        "Preparar presentaci√≥n para el cliente",
        "Lista de compras: manzanas, yogurt, pasta",
        "Ejercicios de matem√°ticas para el examen",
        "Recordatorio: pagar servicios antes del 15"
    ]
    
    categorias_ejemplo = [
        'trabajo', 'personal', 'estudio', 'personal', 'trabajo',
        'trabajo', 'trabajo', 'personal', 'estudio', 'personal'
    ]
    
    # Vectorizar texto
    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
    X_text = vectorizer.fit_transform(notas_ejemplo)
    
    # Entrenar clasificador
    X_train, X_test, y_train, y_test = train_test_split(
        X_text, categorias_ejemplo, test_size=0.3, random_state=42
    )
    
    clasificador = MultinomialNB()
    clasificador.fit(X_train, y_train)
    
    # Predecir categor√≠as
    y_pred = clasificador.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"   üìä Accuracy del clasificador: {accuracy:.3f}")
    print(f"   üìù Ejemplo de predicci√≥n:")
    
    nueva_nota = ["Terminar informe de ventas del trimestre"]
    nueva_prediccion = clasificador.predict(vectorizer.transform(nueva_nota))
    print(f"      Nota: '{nueva_nota[0]}'")
    print(f"      Categor√≠a predicha: {nueva_prediccion[0]}")
    
    # 2. CLUSTERING DE NOTAS SIMILARES
    print("\n2Ô∏è‚É£ CLUSTERING DE NOTAS SIMILARES:")
    
    # Agrupar notas por similitud
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(X_text.toarray())
    
    print(f"   üìä Notas agrupadas en {len(np.unique(clusters))} clusters:")
    
    for i, cluster in enumerate(np.unique(clusters)):
        notas_cluster = [notas_ejemplo[j] for j in range(len(notas_ejemplo)) if clusters[j] == cluster]
        print(f"   üìÅ Cluster {cluster + 1}:")
        for nota in notas_cluster:
            print(f"      ‚Ä¢ {nota}")
    
    # 3. AN√ÅLISIS DE SENTIMIENTOS
    print("\n3Ô∏è‚É£ AN√ÅLISIS DE PATRONES EN NOTAS:")
    
    # Simular caracter√≠sticas de notas
    np.random.seed(42)
    
    datos_notas = {
        'longitud': np.random.normal(100, 30, 50),
        'num_palabras': np.random.normal(20, 8, 50),
        'tiene_fecha': np.random.choice([0, 1], 50, p=[0.7, 0.3]),
        'tiene_numero': np.random.choice([0, 1], 50, p=[0.6, 0.4]),
        'hora_creacion': np.random.randint(0, 24, 50),
        'productividad': np.random.choice(['alta', 'media', 'baja'], 50, p=[0.3, 0.5, 0.2])
    }
    
    df_notas = pd.DataFrame(datos_notas)
    
    # Preparar datos para ML
    X_features = df_notas[['longitud', 'num_palabras', 'tiene_fecha', 'tiene_numero', 'hora_creacion']]
    y_productividad = df_notas['productividad']
    
    # Entrenar modelo para predecir productividad
    from sklearn.ensemble import RandomForestClassifier
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y_productividad, test_size=0.3, random_state=42
    )
    
    modelo_productividad = RandomForestClassifier(random_state=42)
    modelo_productividad.fit(X_train, y_train)
    
    y_pred_prod = modelo_productividad.predict(X_test)
    accuracy_prod = accuracy_score(y_test, y_pred_prod)
    
    print(f"   üìä Accuracy predicci√≥n productividad: {accuracy_prod:.3f}")
    
    # Importancia de caracter√≠sticas
    feature_names = X_features.columns
    importances = modelo_productividad.feature_importances_
    
    print(f"   üìä Caracter√≠sticas m√°s importantes para productividad:")
    for feature, importance in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):
        print(f"      {feature}: {importance:.3f}")
    
    # 4. EJEMPLOS DE FUNCIONES √öTILES
    print("\n4Ô∏è‚É£ FUNCIONES √öTILES PARA TU PROYECTO:")
    
    print("""
def classify_note_category(note_text, vectorizer, classifier):
    '''Clasificar autom√°ticamente una nota'''
    text_vector = vectorizer.transform([note_text])
    category = classifier.predict(text_vector)[0]
    confidence = classifier.predict_proba(text_vector).max()
    return category, confidence

def find_similar_notes(new_note, notes_corpus, vectorizer, n_similar=5):
    '''Encontrar notas similares usando TF-IDF'''
    from sklearn.metrics.pairwise import cosine_similarity
    
    # Vectorizar todas las notas
    all_vectors = vectorizer.fit_transform(notes_corpus + [new_note])
    
    # Calcular similitud
    similarities = cosine_similarity(all_vectors[-1:], all_vectors[:-1])
    similar_indices = similarities[0].argsort()[-n_similar:][::-1]
    
    return [(notes_corpus[i], similarities[0][i]) for i in similar_indices]

def predict_note_productivity(note_features, model):
    '''Predecir si una nota ser√° productiva'''
    # note_features: [longitud, num_palabras, tiene_fecha, tiene_numero, hora]
    productivity = model.predict([note_features])[0]
    confidence = model.predict_proba([note_features]).max()
    return productivity, confidence

def cluster_notes_by_topic(notes_text, n_clusters=5):
    '''Agrupar notas por temas similares'''
    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
    text_vectors = vectorizer.fit_transform(notes_text)
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(text_vectors)
    
    return clusters, vectorizer, kmeans
    """)

def casos_uso_comunes():
    """Casos de uso m√°s comunes de scikit-learn"""
    
    print("\n" + "=" * 50)
    print("üéØ CASOS DE USO COMUNES")
    print("=" * 50)
    
    print("1. Flujo b√°sico de ML:")
    print("   from sklearn.model_selection import train_test_split")
    print("   X_train, X_test, y_train, y_test = train_test_split(X, y)")
    print("   model.fit(X_train, y_train)")
    print("   y_pred = model.predict(X_test)")
    
    print("\n2. Clasificaci√≥n:")
    print("   from sklearn.ensemble import RandomForestClassifier")
    print("   from sklearn.linear_model import LogisticRegression")
    print("   from sklearn.svm import SVC")
    
    print("\n3. Regresi√≥n:")
    print("   from sklearn.linear_model import LinearRegression")
    print("   from sklearn.ensemble import RandomForestRegressor")
    print("   from sklearn.svm import SVR")
    
    print("\n4. Clustering:")
    print("   from sklearn.cluster import KMeans, DBSCAN")
    print("   clusters = KMeans(n_clusters=3).fit_predict(X)")
    
    print("\n5. Preprocessing:")
    print("   from sklearn.preprocessing import StandardScaler")
    print("   scaler = StandardScaler()")
    print("   X_scaled = scaler.fit_transform(X)")
    
    print("\n6. Evaluaci√≥n:")
    print("   from sklearn.metrics import accuracy_score, classification_report")
    print("   from sklearn.model_selection import cross_val_score")
    
    print("\n7. Pipelines:")
    print("   from sklearn.pipeline import Pipeline")
    print("   pipe = Pipeline([('scaler', StandardScaler()), ('clf', RandomForestClassifier())])")

if __name__ == "__main__":
    # Ejecutar todos los ejemplos
    ejemplo_sklearn_basico()
    ejemplo_sklearn_clasificacion()
    ejemplo_sklearn_preprocessing()
    ejemplo_sklearn_clustering()
    ejemplo_sklearn_validacion()
    casos_uso_comunes()
    integracion_con_notesassistant()
    
    print("\n" + "=" * 50)
    print("‚úÖ RESUMEN DEL M√ìDULO scikit-learn")
    print("=" * 50)
    print("üîß Usos principales:")
    print("   ‚Ä¢ Clasificaci√≥n y regresi√≥n")
    print("   ‚Ä¢ Clustering y reducci√≥n dimensional")
    print("   ‚Ä¢ Preprocessing de datos")
    print("   ‚Ä¢ Validaci√≥n y selecci√≥n de modelos")
    print("   ‚Ä¢ M√©tricas de evaluaci√≥n")
    print("   ‚Ä¢ Pipelines de ML completos")
    print("\nüìö Documentaci√≥n oficial:")
    print("   https://scikit-learn.org/")
    print("\nüí° Consejo: scikit-learn es la puerta de entrada al ML")
    print("   API consistente, documentaci√≥n excelente, comunidad activa.") 